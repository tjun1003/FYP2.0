{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "38787269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ğŸ”§ è½½å…¥ç¯å¢ƒ ---\n",
    "from pathlib import Path\n",
    "from Backend.Inference import ModelWrapper, ModelConfig  # â† æ”¹æˆä½ é¡¹ç›®çš„æ¨¡å—è·¯å¾„\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "72779e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-07 00:06:58,667 - Backend.Inference - INFO - ğŸš€ å¼€å§‹åŠ è½½æ¨¡å‹...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backend\\saved_models\\model_20251007_123355.h5\n",
      "Backend\\saved_models\\id2label_20251007_123355.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-07 00:06:59,160 - absl - WARNING - Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "2025-11-07 00:06:59,179 - Backend.Inference - INFO - ğŸ”¥ åŠ è½½ MiniLM ç¼–ç å™¨...\n",
      "2025-11-07 00:06:59,180 - Backend.Inference - INFO -    ä» Hugging Face åŠ è½½: sentence-transformers/all-MiniLM-L6-v2\n",
      "2025-11-07 00:06:59,184 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu\n",
      "2025-11-07 00:06:59,184 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n",
      "2025-11-07 00:07:02,677 - Backend.Inference - INFO - âœ… MiniLM ç¼–ç å™¨åŠ è½½æˆåŠŸ\n",
      "2025-11-07 00:07:02,677 - Backend.Inference - INFO - ğŸ”¥ åŠ è½½ BART ç¼–ç å™¨...\n",
      "2025-11-07 00:07:02,678 - Backend.Inference - INFO -    ä» Hugging Face åŠ è½½: facebook/bart-large\n",
      "2025-11-07 00:07:02,678 - Backend.Inference - INFO -    ä½¿ç”¨é»˜è®¤ç¼“å­˜: ~/.cache/huggingface/\n",
      "2025-11-07 00:07:02,682 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu\n",
      "2025-11-07 00:07:02,683 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: facebook/bart-large\n",
      "2025-11-07 00:07:02,956 - sentence_transformers.SentenceTransformer - WARNING - No sentence-transformers model found with name facebook/bart-large. Creating a new one with mean pooling.\n",
      "2025-11-07 00:07:06,791 - Backend.Inference - INFO - âœ… BART ç¼–ç å™¨åŠ è½½æˆåŠŸ\n",
      "2025-11-07 00:07:06,792 - Backend.Inference - INFO - ğŸ”¥ åŠ è½½ RoBERTa ç¼–ç å™¨...\n",
      "2025-11-07 00:07:06,793 - Backend.Inference - INFO -    ä» Hugging Face åŠ è½½: roberta-base\n",
      "2025-11-07 00:07:06,793 - Backend.Inference - INFO -    ä½¿ç”¨é»˜è®¤ç¼“å­˜: ~/.cache/huggingface/\n",
      "Some weights of BertForTextRepresentation were not initialized from the model checkpoint at roberta-base and are newly initialized: ['bert.embeddings.LayerNorm.bias', 'bert.embeddings.LayerNorm.weight', 'bert.embeddings.position_embeddings.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.embeddings.word_embeddings.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-11-07 00:07:08,709 - Backend.Inference - INFO - âœ… RoBERTa ç¼–ç å™¨åŠ è½½æˆåŠŸ\n",
      "2025-11-07 00:07:08,710 - Backend.Inference - INFO - âœ… æ¨¡å‹ä¸ç¼–ç å™¨åŠ è½½å®Œæˆ!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼\n"
     ]
    }
   ],
   "source": [
    "# è·å–å½“å‰ç›®å½•\n",
    "base_dir = Path(__file__).parent if \"__file__\" in locals() else Path(\".\")\n",
    "\n",
    "# æŒ‡å‘ Backend ç›®å½•\n",
    "backend_dir = base_dir / \"Backend\"\n",
    "\n",
    "# æ¨¡å‹æ–‡ä»¶è·¯å¾„\n",
    "model_path = backend_dir / \"saved_models\" / \"model_20251007_123355.h5\"\n",
    "label_path = backend_dir / \"saved_models\" / \"id2label_20251007_123355.pkl\"\n",
    "\n",
    "print(model_path)\n",
    "print(label_path)\n",
    "\n",
    "# âœ… åˆå§‹åŒ–é…ç½®å¯¹è±¡\n",
    "config = ModelConfig()\n",
    "\n",
    "# âœ… åˆ›å»ºæ¨¡å‹åŒ…è£…å™¨å®ä¾‹\n",
    "wrapper = ModelWrapper(\n",
    "    model_path=model_path,\n",
    "    label_path=label_path,\n",
    "    config=config\n",
    ")\n",
    "\n",
    "print(\"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1e5e3e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagnose_input(text):\n",
    "    print(f\"\\nğŸ§© è¾“å…¥å¥å­: {text}\")\n",
    "\n",
    "    # 1ï¸âƒ£ ç”Ÿæˆ BART å‘é‡\n",
    "    bart_emb = SentenceTransformer('facebook/bart-large').encode([text])\n",
    "    bart_emb = bart_emb.reshape((1, 1, bart_emb.shape[1]))  # (1, 1, 1024)\n",
    "\n",
    "    # 2ï¸âƒ£ ç”Ÿæˆ RoBERTa å‘é‡\n",
    "    roberta_emb = RepresentationModel(model_type=\"roberta\", model_name=\"roberta-base\", use_cuda=False)\n",
    "    roberta_emb = roberta_emb.encode_sentences([text], combine_strategy=\"mean\")\n",
    "    roberta_emb = roberta_emb.reshape((1, 1, roberta_emb.shape[1]))  # (1, 1, 768)\n",
    "\n",
    "    print(f\"ğŸ“ BART shape: {bart_emb.shape}, RoBERTa shape: {roberta_emb.shape}\")\n",
    "\n",
    "    # 3ï¸âƒ£ æ¨¡å‹é¢„æµ‹\n",
    "    preds = model.predict([bart_emb, roberta_emb])\n",
    "    label_id = int(np.argmax(preds, axis=1)[0])\n",
    "    print(f\"ğŸ”® æ¨¡å‹é¢„æµ‹æ ‡ç­¾: {id2label[label_id]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "af2243ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ§© è¾“å…¥å¥å­: Mount Everest is the tallest mountain on Earth\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'SentenceTransformer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# --- ğŸ§  æµ‹è¯• ---\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mdiagnose_input\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mMount Everest is the tallest mountain on Earth\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m diagnose_input(\u001b[33m\"\u001b[39m\u001b[33mThe moon is made of cheese\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m diagnose_input(\u001b[33m\"\u001b[39m\u001b[33mDonald Trump is the current president of the USA\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mdiagnose_input\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mğŸ§© è¾“å…¥å¥å­: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# 1ï¸âƒ£ ç”Ÿæˆ BART å‘é‡\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m bart_emb = \u001b[43mSentenceTransformer\u001b[49m(\u001b[33m'\u001b[39m\u001b[33mfacebook/bart-large\u001b[39m\u001b[33m'\u001b[39m).encode([text])\n\u001b[32m      6\u001b[39m bart_emb = bart_emb.reshape((\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m, bart_emb.shape[\u001b[32m1\u001b[39m]))  \u001b[38;5;66;03m# (1, 1, 1024)\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# 2ï¸âƒ£ ç”Ÿæˆ RoBERTa å‘é‡\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'SentenceTransformer' is not defined"
     ]
    }
   ],
   "source": [
    "# --- ğŸ§  æµ‹è¯• ---\n",
    "diagnose_input(\"Mount Everest is the tallest mountain on Earth\")\n",
    "diagnose_input(\"The moon is made of cheese\")\n",
    "diagnose_input(\"Donald Trump is the current president of the USA\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FYP2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
